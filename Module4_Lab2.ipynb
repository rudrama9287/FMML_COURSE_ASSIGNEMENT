{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rudrama9287/FMML_COURSE_ASSIGNEMENT/blob/main/Module4_Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyMhDmOed0RJ"
      },
      "source": [
        "# FOUNDATIONS OF MODERN MACHINE LEARNING, IIIT Hyderabad\n",
        "# Module 4: Perceptron and Gradient Descent\n",
        "## Lab 2: Introduction to Gradient Descent\n",
        "### Module Coordinator: Jashn Arora\n",
        "\n",
        "Gradient descent is a very important algorithm to understand, as it underpins many of the more advanced algorithms used in Machine Learning and Deep Learning.\n",
        "\n",
        "A brief overview of the algorithm is\n",
        "\n",
        "\n",
        "*   start with a random initialization of the solution.\n",
        "*   incrementally change the solution by moving in the direction of negative gradient of the objective function.\n",
        "*   repeat the previous step until some convergence criteria is met.\n",
        "\n",
        "The key equation for change in weight is:\n",
        "$$w^{k+1} \\leftarrow w^k - \\eta \\Delta J$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx5OzL5jbnkO"
      },
      "source": [
        "# Importing the required libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random   \n",
        "\n",
        "random.seed(42)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQpDHGOAh0It"
      },
      "source": [
        "We can start be choosing coefficients for a second degree polynomial equation $(a x^2 + bx + c)$ that will distribute the data we will try to model.\n",
        "\n",
        "Let's define some random x data (inputs) we hope to predict y (outputs) of."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnbvlEbWcUtM"
      },
      "source": [
        "def eval_2nd_degree(coeffs, x):\n",
        "    \"\"\"\n",
        "    Function to return the output of evaluating a second degree polynomial,\n",
        "    given a specific x value.\n",
        "    \n",
        "    Args:\n",
        "        coeffs: List containing the coefficients a, b, and c for the polynomial.\n",
        "        x: The input x value to the polynomial.\n",
        "    \n",
        "    Returns:\n",
        "        y: The corresponding output y value for the second degree polynomial.\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    a = coeffs[0] * (x * x)\n",
        "    b = coeffs[1] * x\n",
        "    c = coeffs[2]\n",
        "    y = a + b + c\n",
        "    return y\n",
        "\n",
        "hundred_xs = np.random.uniform(-10, 10, 100)\n",
        "coeffs = [1, 0, 0]\n",
        "\n",
        "xs = []\n",
        "ys = []\n",
        "for x in hundred_xs:\n",
        "    y  = eval_2nd_degree(coeffs, x)\n",
        "    xs.append(x)\n",
        "    ys.append(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a-Tzv5fclE2"
      },
      "source": [
        "plt.plot(xs, ys, 'g+')\n",
        "plt.title('Original data')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQr81EuciKhB"
      },
      "source": [
        "This is good, but we could improve on this by making things more realistic. You can add noise or **jitter** to the values so they can resemble real-world data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggni_nKPdFZ5"
      },
      "source": [
        "def eval_2nd_degree_jitter(coeffs, x, j):\n",
        "    \"\"\"\n",
        "    Function to return the noisy output of evaluating a second degree polynomial,\n",
        "    given a specific x value. Output values can be within [y − j, y + j].\n",
        "    \n",
        "    Args:\n",
        "        coeffs: List containing the coefficients a, b, and c for the polynomial.\n",
        "        x: The input x value to the polynomial.\n",
        "        j: Jitter parameter, to introduce noise to output y.\n",
        "    \n",
        "    Returns:\n",
        "        y: The corresponding jittered output y value for the second degree polynomial.\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    a = coeffs[0] * (x * x)\n",
        "    b = coeffs[1] * x\n",
        "    c = coeffs[2]\n",
        "    y = a + b + c\n",
        "    \n",
        "    interval = [y - j, y + j]\n",
        "    interval_min = interval[0]\n",
        "    interval_max = interval[1]\n",
        "    jit_val = random.random() * interval_max      # Generate a random number in range 0 to interval max \n",
        "    \n",
        "    while interval_min > jit_val:                 # While the random jitter value is less than the interval min,\n",
        "        jit_val = random.random() * interval_max  # it is not in the right range. Re-roll the generator until it \n",
        "                                                  # give a number greater than the interval min. \n",
        "    \n",
        "    return jit_val\n",
        "\n",
        "xs = []\n",
        "ys = []\n",
        "for x in hundred_xs:\n",
        "    y  = eval_2nd_degree_jitter(coeffs, x, 0.1)\n",
        "    xs.append(x)\n",
        "    ys.append(y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFYv43vpe5Y4"
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(xs, ys, 'g+')\n",
        "plt.title('Original data with jitter')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umByA5Ghi_gt"
      },
      "source": [
        "We will now build our predictive model, and optimize it with gradient descent and we will try to get as close to these values as possible.\n",
        "\n",
        "To get a quantifiable measure of how incorrect it is, we calculate the Mean Squared Error loss for the model. This is the mean value of the sum of the squared differences between the actual and predicted outputs.\n",
        "\n",
        "$$ E = \\frac{1}{n} \\sum_{i=0}^n (y_i - \\bar{y_i})^2 $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGo9VtQDfG6F"
      },
      "source": [
        "def loss_mse(ys, y_bar):\n",
        "    \"\"\"\n",
        "    Calculates MSE loss.\n",
        "    \n",
        "    Args:\n",
        "        ys: training data labels\n",
        "        y_bar: prediction labels\n",
        "    \n",
        "    Returns: Calculated MSE loss.\n",
        "    \"\"\"\n",
        "\n",
        "    return sum((ys - y_bar) * (ys - y_bar)) / len(ys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIRquRB3kcZA"
      },
      "source": [
        "rand_coeffs = (random.randrange(-10, 10), random.randrange(-10, 10), random.randrange(-10, 10))\n",
        "y_bar = eval_2nd_degree(rand_coeffs, hundred_xs)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(xs, ys, 'g+', label = 'original')\n",
        "plt.plot(xs, y_bar, 'ro', label = 'prediction')\n",
        "plt.title('Original data vs first prediction')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbwBb4Ckomw"
      },
      "source": [
        "initial_model_loss = loss_mse(ys, y_bar)\n",
        "initial_model_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEcvjxbJa8cq"
      },
      "source": [
        "We can see that the loss is quite a large number. Let’s now see if we can improve on this fairly high loss metric by optimizing the model with gradient descent.\n",
        "\n",
        "We wish to improve our model. Therefore we want to alter its coefficients $a$, $b$ and $c$ to decrease the error. Therefore we require knowledge about how each coefficient affects the error. This is achieved by calculating the partial derivative of the loss function with respect to **each** of the individual coefficients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhiloANqkSFc"
      },
      "source": [
        "def calc_gradient_2nd_poly(rand_coeffs, hundred_xs, ys): \n",
        "    \"\"\"\n",
        "    calculates the gradient for a second degree polynomial.\n",
        "    \n",
        "    Args:\n",
        "        coeffs: a,b and c, for a 2nd degree polynomial [ y = ax^2 + bx + c ]\n",
        "        inputs_x: x input datapoints\n",
        "        outputs_y: actual y output points\n",
        "        \n",
        "    Returns: Calculated gradients for the 2nd degree polynomial, as a tuple of its parts for a,b,c respectively.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    a_s = []\n",
        "    b_s = []\n",
        "    c_s = []\n",
        "    \n",
        "    y_bars = eval_2nd_degree(rand_coeffs, hundred_xs)\n",
        "    \n",
        "    for x, y, y_bar in list(zip(hundred_xs, ys, y_bars)):    # take tuple of (x datapoint, actual y label, predicted y label)\n",
        "        x_squared = x ** 2\n",
        "        partial_a = x_squared * (y - y_bar)\n",
        "        a_s.append(partial_a)\n",
        "        partial_b = x * (y - y_bar)\n",
        "        b_s.append(partial_b)\n",
        "        partial_c = (y - y_bar)\n",
        "        c_s.append(partial_c)\n",
        "    \n",
        "    num = [i for i in y_bars]\n",
        "    n = len(num)\n",
        "    \n",
        "    gradient_a = (-2 / n) * sum(a_s)\n",
        "    gradient_b = (-2 / n) * sum(b_s)\n",
        "    gradient_c = (-2 / n) * sum(c_s)\n",
        "\n",
        "    return (gradient_a, gradient_b, gradient_c)   # return calculated gradients as a a tuple of its 3 parts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN0jR2Dhkpjn"
      },
      "source": [
        "calc_grad = calc_gradient_2nd_poly(rand_coeffs, hundred_xs, ys)\n",
        "\n",
        "lr = 0.0001\n",
        "a_new = rand_coeffs[0] - lr * calc_grad[0]\n",
        "b_new = rand_coeffs[1] - lr * calc_grad[1]\n",
        "c_new = rand_coeffs[2] - lr * calc_grad[2]\n",
        "\n",
        "new_model_coeffs = (a_new, b_new, c_new)\n",
        "print(f\"New model coeffs: {new_model_coeffs}\")\n",
        "\n",
        "# update with these new coeffs:\n",
        "new_y_bar = eval_2nd_degree(new_model_coeffs, hundred_xs)\n",
        "updated_model_loss = loss_mse(ys, new_y_bar)\n",
        "\n",
        "print(f\"Now have smaller model loss: {updated_model_loss} vs {initial_model_loss}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rjqrqclk4BI"
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(xs, ys, 'g+', label = 'original model')\n",
        "plt.plot(xs, y_bar, 'ro', label = 'first prediction')\n",
        "plt.plot(xs, new_y_bar, 'b.', label = 'updated prediction')\n",
        "plt.title('Original model vs 1st prediction vs updated prediction with lower loss')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOzSlzJIfvid"
      },
      "source": [
        "We’re almost ready. The last step will be to perform gradient descent iteratively over a number of epochs (cycles or iterations.) With every epoch we hope to see an improvement in the form of lowered loss, and better model-fitting to the original data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBkU4dRnlHKy"
      },
      "source": [
        "def calc_gradient_2nd_poly_for_GD(coeffs, inputs_x, outputs_y, lr): \n",
        "    \"\"\"\n",
        "    calculates the gradient for a second degree polynomial.\n",
        "    \n",
        "    Args:\n",
        "        coeffs: a,b and c, for a 2nd degree polynomial [ y = ax^2 + bx + c ]\n",
        "        inputs_x: x input datapoints\n",
        "        outputs_y: actual y output points\n",
        "        lr: learning rate\n",
        "        \n",
        "    Returns: Calculated gradients for the 2nd degree polynomial, as a tuple of its parts for a,b,c respectively.\n",
        "    \n",
        "    \"\"\"\n",
        "    a_s = []\n",
        "    b_s = []\n",
        "    c_s = []\n",
        "        \n",
        "    y_bars = eval_2nd_degree(coeffs, inputs_x)\n",
        "\n",
        "    for x,y,y_bar in list(zip(inputs_x, outputs_y, y_bars)):    # take tuple of (x datapoint, actual y label, predicted y label)\n",
        "        x_squared = x ** 2        \n",
        "        partial_a = x_squared * (y - y_bar)\n",
        "        a_s.append(partial_a)\n",
        "        partial_b = x * (y - y_bar)\n",
        "        b_s.append(partial_b)\n",
        "        partial_c = (y - y_bar)\n",
        "        c_s.append(partial_c)\n",
        "    \n",
        "    num = [i for i in y_bars]\n",
        "    n = len(num)\n",
        "    \n",
        "    gradient_a = (-2 / n) * sum(a_s)\n",
        "    gradient_b = (-2 / n) * sum(b_s)\n",
        "    gradient_c = (-2 / n) * sum(c_s)\n",
        "\n",
        "\n",
        "    a_new = coeffs[0] - lr * gradient_a\n",
        "    b_new = coeffs[1] - lr * gradient_b\n",
        "    c_new = coeffs[2] - lr * gradient_c\n",
        "    \n",
        "    new_model_coeffs = (a_new, b_new, c_new)\n",
        "    \n",
        "    # update with these new coeffs:\n",
        "    new_y_bar = eval_2nd_degree(new_model_coeffs, inputs_x)\n",
        "    \n",
        "    updated_model_loss = loss_mse(outputs_y, new_y_bar)\n",
        "    return updated_model_loss, new_model_coeffs, new_y_bar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj6K6SXol_bi"
      },
      "source": [
        "def gradient_descent(epochs, lr):\n",
        "    \"\"\"\n",
        "    Perform gradient descent for a second degree polynomial.\n",
        "    \n",
        "    Args:\n",
        "        epochs: number of iterations to perform of finding new coefficients and updatingt loss. \n",
        "        lr: specified learning rate\n",
        "        \n",
        "    Returns: Tuple containing (updated_model_loss, new_model_coeffs, new_y_bar predictions, saved loss updates)\n",
        "    \n",
        "    \"\"\"\n",
        "    losses = []\n",
        "    rand_coeffs_to_test = rand_coeffs\n",
        "    for i in range(epochs):\n",
        "        loss = calc_gradient_2nd_poly_for_GD(rand_coeffs_to_test, hundred_xs, ys, lr)\n",
        "        rand_coeffs_to_test = loss[1]\n",
        "        losses.append(loss[0])\n",
        "    #print(losses)\n",
        "    return loss[0], loss[1], loss[2], losses  # (updated_model_loss, new_model_coeffs, new_y_bar, saved loss updates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brk2qRFlmAQM"
      },
      "source": [
        "GD = gradient_descent(30000, 0.0003)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(xs, ys, 'g+', label = 'original')\n",
        "plt.plot(xs, GD[2], 'b.', label = 'final_prediction')\n",
        "plt.title('Original vs Final prediction after Gradient Descent')\n",
        "plt.legend(loc = \"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS2KZ6SxfnAI"
      },
      "source": [
        "This trained model is showing vast improvements after it’s full training cycle. We can examine further by inspecting its final predicted coefficients $a$, $b$ and $c$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efY8ehhvmCRz"
      },
      "source": [
        "print(f\"Final Coefficients predicted: {GD[1]}\")\n",
        "print(f\"Original Coefficients: {coeffs}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8PuwB87fjP5"
      },
      "source": [
        "Not too far off! A big improvement over the initial random model. Looking at the plot of the loss reduction over training offers further insights.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnswAURtmFBG"
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(GD[3], 'b-', label = 'loss')\n",
        "plt.title('Loss over 500 iterations')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('MSE')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu7fnsphdJpo"
      },
      "source": [
        "We observe that the model loss reached close to zero, to give us our more accurate coefficients. We can also see there was no major improvement in loss after about 100 epochs. An alternative strategy would be to add some kind of condition to the training step that stops training when a certain minimum loss threshold has been reached. This would prevent excessive training and potential over-fitting for the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3oxAVdtePYa"
      },
      "source": [
        "# Things to try\n",
        "\n",
        "\n",
        "\n",
        "1.   Change the coefficients array and try a different polynomial instead of our $x^2$.\n",
        "2.   Increase/decrease the learning rate to see how many iterations will be take to coverge. Does it even converge on a huge learning rate?\n",
        "3. Take a degree 5 polynomial with 5 roots and try different initializations, instead of random ones. Does it converge on different values for different initializations? Why does initialization not matter in our case of $x^2$?\n",
        "4. Can you modify the algorithm to find a maxima of a function, instead of a minima?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANSWER FOR QUESTION NO: 1\n",
        "\n"
      ],
      "metadata": {
        "id": "SIKsMdGiDtCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_2nd_degree(coeffs, x):\n",
        "    \"\"\"\n",
        "    Here i had taken x**3 polynomial equation \n",
        "    and changed coefficients to [3,0,5,0]\n",
        "    \"\"\"\n",
        "\n",
        "    a = coeffs[0] * (x * x * x)\n",
        "    b = coeffs[1] * (x * x)\n",
        "    c = coeffs[2] * x\n",
        "    d = coeffs[3]\n",
        "    y = a + b + c + d\n",
        "    return y\n",
        "\n",
        "hundred_xs = np.random.uniform(-10, 10, 100)\n",
        "coeffs = [3,0,5,0]\n",
        "\n",
        "xs = []\n",
        "ys = []\n",
        "for x in hundred_xs:\n",
        "    y  = eval_2nd_degree(coeffs, x)\n",
        "    xs.append(x)\n",
        "    ys.append(y)"
      ],
      "metadata": {
        "id": "boET6n2tDt3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(xs, ys, 'g+')\n",
        "plt.title('Original data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3femz2tOEBlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANSWER FOR QUESTION NO: 2"
      ],
      "metadata": {
        "id": "XQotoxkouttY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_2nd_degree(coeffs, x):\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    a = coeffs[0] * (x * x)\n",
        "    b = coeffs[1] * x\n",
        "    c = coeffs[2]\n",
        "    y = a + b + c\n",
        "    return y\n",
        "\n",
        "hundred_xs = np.random.uniform(-10, 10, 100)\n",
        "coeffs = [1, 0, 0]\n",
        "\n",
        "xs = []\n",
        "ys = []\n",
        "for x in hundred_xs:\n",
        "    y  = eval_2nd_degree(coeffs, x)\n",
        "    xs.append(x)\n",
        "    ys.append(y)"
      ],
      "metadata": {
        "id": "3-b-92tqFBJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# At lr = -1\n",
        "GD = gradient_descent(30000, -1)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(xs, ys, 'g+', label = 'original')\n",
        "plt.plot(xs, GD[2], 'b.', label = 'final_prediction')\n",
        "plt.title('Original vs Final prediction after Gradient Descent')\n",
        "plt.legend(loc = \"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hDj_uFRHEGH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#At lr = 0.0004\n",
        "GD = gradient_descent(30000, 0.0004)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(xs, ys, 'g+', label = 'original')\n",
        "plt.plot(xs, GD[2], 'b.', label = 'final_prediction')\n",
        "plt.title('Original vs Final prediction after Gradient Descent')\n",
        "plt.legend(loc = \"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KAUlwi_tErcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANSWER FOR QUESTION NO: 3\n"
      ],
      "metadata": {
        "id": "MLUYFFC6QnVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_2nd_degree(coeffs, x):\n",
        "    \"\"\"\n",
        "    Here i had taken x**5 polynomial equation \n",
        "    and changed coefficients to [3,0,5,0,4,0]\n",
        "    \"\"\"\n",
        "\n",
        "    a = coeffs[0] * (x * x * x * x * x)\n",
        "    b = coeffs[1] * (x * x * x * x)\n",
        "    c = coeffs[2] * (x * x * x)\n",
        "    d = coeffs[3] * (x * x)\n",
        "    e = coeffs[4] * x\n",
        "    f = coeffs[5]\n",
        "    y = a + b + c + d + e + f\n",
        "    return y\n",
        "\n",
        "hundred_xs = np.random.uniform(-1, 10, 100)\n",
        "coeffs = [3,0,5,0,4,0]\n",
        "\n",
        "xs = []\n",
        "ys = []\n",
        "for x in hundred_xs:\n",
        "    y  = eval_2nd_degree(coeffs, x)\n",
        "    xs.append(x)\n",
        "    ys.append(y)"
      ],
      "metadata": {
        "id": "HQhsDGm6FhI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(xs, ys, 'g+')\n",
        "plt.title('Original data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ig5sUrooRIgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANSWER FOR QUESTION NO :4"
      ],
      "metadata": {
        "id": "qVFLN2Vqt-qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4...............\n",
        "\"\"\"\n",
        "As i searched in google i had seen that by changing the signs to positive gives the maxima function instead of minima . \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xH-cQe4FRPzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_gradient_2nd_poly_for_GD(coeffs, inputs_x, outputs_y, lr): \n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "    a_s = []\n",
        "    b_s = []\n",
        "    c_s = []\n",
        "        \n",
        "    y_bars = eval_2nd_degree(coeffs, inputs_x)\n",
        "\n",
        "    for x,y,y_bar in list(zip(inputs_x, outputs_y, y_bars)):    # take tuple of (x datapoint, actual y label, predicted y label)\n",
        "        x_squared = x ** 2        \n",
        "        partial_a = x_squared * (y + y_bar)\n",
        "        a_s.append(partial_a)\n",
        "        partial_b = x * (y + y_bar)\n",
        "        b_s.append(partial_b)\n",
        "        partial_c = (y + y_bar)\n",
        "        c_s.append(partial_c)\n",
        "    \n",
        "    num = [i for i in y_bars]\n",
        "    n = len(num)\n",
        "    \n",
        "    gradient_a = (2 / n) * sum(a_s)\n",
        "    gradient_b = (2 / n) * sum(b_s)\n",
        "    gradient_c = (2 / n) * sum(c_s)\n",
        "\n",
        "\n",
        "    a_new = coeffs[0] + lr * gradient_a\n",
        "    b_new = coeffs[1] + lr * gradient_b\n",
        "    c_new = coeffs[2] + lr * gradient_c\n",
        "    \n",
        "    new_model_coeffs = (a_new, b_new, c_new)\n",
        "    \n",
        "    new_y_bar = eval_2nd_degree(new_model_coeffs, inputs_x)\n",
        "    \n",
        "    updated_model_loss = loss_mse(outputs_y, new_y_bar)\n",
        "    return updated_model_loss, new_model_coeffs, new_y_bar"
      ],
      "metadata": {
        "id": "10M9RVEPdAFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_mse(ys, y_bar):\n",
        "    \"\"\"\n",
        "    Calculates MSE loss.\n",
        "    \n",
        "    Args:\n",
        "        ys: training data labels\n",
        "        y_bar: prediction labels\n",
        "    \n",
        "    Returns: Calculated MSE loss.\n",
        "    \"\"\"\n",
        "\n",
        "    return sum((ys - y_bar) * (ys - y_bar)) / len(ys)"
      ],
      "metadata": {
        "id": "7arfOOWGl2_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(epochs, lr):\n",
        "    \n",
        "    losses = []\n",
        "    rand_coeffs_to_test = rand_coeffs\n",
        "    for i in range(epochs):\n",
        "        loss = calc_gradient_2nd_poly_for_GD(rand_coeffs_to_test, hundred_xs, ys, lr)\n",
        "        rand_coeffs_to_test = loss[1]\n",
        "        losses.append(loss[0])\n",
        "    print(losses)\n",
        "    return loss[0], loss[1], loss[2], losses  "
      ],
      "metadata": {
        "id": "JQaH85sHdIbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(GD[3],  label = 'loss')\n",
        "#plt.title('Loss over 500 iterations')\n",
        "#plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lm4i2jjndNzK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}